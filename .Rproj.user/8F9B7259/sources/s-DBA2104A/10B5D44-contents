---
title: Scraping a News Website
author: Scott Lee
date: '2019-06-20'
categories:
  - R
  - Rvest
slug: first-post-basic-webscraping
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

**Some Context**

One day after work, I was messaging with a friend (who's extremely curious about data and it's applications and a source of great ideas) and we were talking about scraping data from the internet and what tools were currently available and most easy to learn. R and Python sprang to mind and so we deep dived into some of the packages available in those languages. Then, funnily enough, the conversation turned to VBA and crude methods such as using sendkeys and how unreliable they were.  

So, inspired as I usually am, I got home that night, turned my PC on and decided to scrap data from a couple of websites using R. 

*Please always be mindful of the legality of webscraping when you webscrape, useful link [here](https://www.datahen.com/data-crawling-get-legal-side/?utm_source=Quora&utm_medium=Answers).*

**My Workflow**

* I would say the package widely used in R for webscraping is [Rvest](https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/). These are the packages I used in my workflow:
  + *rvest*
  + *lubridate*
  + *dplyr*
  + *gmailr*
* As the link to the Rvest blog points out, using [selectorgadget](https://selectorgadget.com/) (a Chrome extension) makes finding the content you want to extract that much easier. It is possible without it but you would have to spend countless minutes trying to locate the css selector you are after - not fun at all! 
* 

