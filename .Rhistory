blogdown::new_site()
blogdown::serve_site()
blogdown:::update_meta_addin()
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::update_meta_addin()
blogdown:::insert_image_addin()
?html_text
library(rvest)
?html_text
efinancial_news <- read_html("https://news.efinancialcareers.com/hk-en/en/news-analysis")
top_news <- efinancial_news %>%
html_nodes(".mt-0 a") %>%
html_text()
?html_attr
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown::serve_site()
install.packages("Rlinkedin")
library(openxlsx)
install.packages("openxlsx")
blogdown::serve_site()
blogdown::serve_site()
blogdown:::new_post_addin()
blogdown:::update_meta_addin()
a
knitr::opts_chunk$set(collapse = TRUE)
library(readr)
library(dplyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
data <- read_csv("seek_australia_sample.csv",locale = readr::locale(encoding = "windows-1252"))
data_flagged <- data %>%
mutate(flag=ifelse(category %in% c("Healthcare & Medical"),"Yes","No")) %>%
mutate(no_chars=nchar(job_description))
descriptions_corpus <- VCorpus(VectorSource(data_flagged$job_description)) ##supply job descriptions into VCorpus
descriptions_corpus_clean <- tm_map(descriptions_corpus,content_transformer(tolower)) ##text transformation - lowercase
descriptions_corpus_clean <- tm_map(descriptions_corpus_clean, removeNumbers) ##remove numbers
descriptions_corpus_clean <- tm_map(descriptions_corpus_clean,removeWords, stopwords()) ##list of stopwords listed in the tm package
descriptions_corpus_clean <- tm_map(descriptions_corpus_clean, removePunctuation)
descriptions_corpus_clean <- tm_map(descriptions_corpus_clean, stripWhitespace)
descriptions_dtm <- DocumentTermMatrix(descriptions_corpus_clean) ##this will create a sparse matrix with all the words found in the above having its own column with the number of its occurrence for each job description listed in the cell
813+75
888/945
813+58
871/945
blogdown:::new_post_addin()
blogdown::serve_site()
install.packages("networkD3")
library(networkD3)
data(MisLinks)
data(MisNodes)
library(networkD3)
data(MisLinks)
data(MisNodes)
forceNetwork(Links = MisLinks, Nodes = MisNodes, Source = "source",
Target = "target", Value = "value", NodeID = "name",
Group = "group", opacity = 1)
library(htmltools)
htmltools::includeHTML("new 2.html")
t <-readLines("new 2.html")
len <- length(t)-1
cat(t[2:len])
blogdown::serve_site()
install.packages("blogdown")
blogdown::serve_site()
blogdown:::new_post_addin()
knitr::opts_chunk$set(collapse = TRUE)
library(keras)
library(stringr)
library(readr)
text_dump <- read_csv("Texas Last Statement - CSV.csv",locale = readr::locale(encoding = "windows-1252"))
text_collapsed <- paste(text$LastStatement,collapse=" ")
setwd("C:/Users/scott/Documents/Projects")
text_dump <- read_csv("Texas Last Statement - CSV.csv",locale = readr::locale(encoding = "windows-1252"))
text_collapsed <- paste(text$LastStatement,collapse=" ")
text <- str_to_lower(text_collapsed)
# text <- tolower(readChar("bezos.txt",nchars = 1000000))
cat("Corpus length:",nchar(text_lowered),"\n")
maxlen <- 60
step <- 3
text_indexes <- seq(1,nchar(text) - maxlen,by=step)
sentences <-  str_sub(text,text_indexes,text_indexes + maxlen - 1)
next_chars <-  str_sub(text,text_indexes + maxlen, text_indexes + maxlen)
cat("Number of sequences: ",length(sentences),"\n")
chars <- unique(sort(strsplit(text,"")[[1]]))
cat("Unique characters:",length(chars),"\n")
char_indices <- 1:length(chars)
names(char_indices) <- chars
x <- array(0L,dim=c(length(sentences),maxlen,length(chars)))
y <- array(0L,dim=c(length(sentences),length(chars)))
for (i in 1:length(sentences)){
sentence <- strsplit(sentences[[i]],"")[[1]]
for (t in 1:length(sentence)) {
char <- sentence[[t]]
x[i,t,char_indices[[char]]] <- 1
}
next_char <- next_chars[[i]]
y[i,char_indices[[next_char]]] <- 1
}
model <- keras_model_sequential() %>%
layer_lstm(units=128,input_shape = c(maxlen,length(chars))) %>%
layer_dense(units=length(chars),activation="softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
sample_next_char <- function(preds,temperature = 1.0) {
preds <- as.numeric(preds)
preds <-  log(preds)/temperature
exp_preds <- exp(preds)
preds <- exp_preds/sum(exp_preds)
which.max(t(rmultinom(1,1,preds)))
}
setwd("C:/Users/scott/Documents/Projects")
text_dump <- read_csv("Texas Last Statement - CSV.csv",locale = readr::locale(encoding = "windows-1252"))
text_collapsed <- paste(text_dump$LastStatement,collapse=" ")
text <- str_to_lower(text_collapsed)
# text <- tolower(readChar("bezos.txt",nchars = 1000000))
cat("Corpus length:",nchar(text_lowered),"\n")
maxlen <- 60
step <- 3
text_indexes <- seq(1,nchar(text) - maxlen,by=step)
sentences <-  str_sub(text,text_indexes,text_indexes + maxlen - 1)
next_chars <-  str_sub(text,text_indexes + maxlen, text_indexes + maxlen)
cat("Number of sequences: ",length(sentences),"\n")
chars <- unique(sort(strsplit(text,"")[[1]]))
cat("Unique characters:",length(chars),"\n")
char_indices <- 1:length(chars)
names(char_indices) <- chars
x <- array(0L,dim=c(length(sentences),maxlen,length(chars)))
y <- array(0L,dim=c(length(sentences),length(chars)))
for (i in 1:length(sentences)){
sentence <- strsplit(sentences[[i]],"")[[1]]
for (t in 1:length(sentence)) {
char <- sentence[[t]]
x[i,t,char_indices[[char]]] <- 1
}
next_char <- next_chars[[i]]
y[i,char_indices[[next_char]]] <- 1
}
model <- keras_model_sequential() %>%
layer_lstm(units=128,input_shape = c(maxlen,length(chars))) %>%
layer_dense(units=length(chars),activation="softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
sample_next_char <- function(preds,temperature = 1.0) {
preds <- as.numeric(preds)
preds <-  log(preds)/temperature
exp_preds <- exp(preds)
preds <- exp_preds/sum(exp_preds)
which.max(t(rmultinom(1,1,preds)))
}
for (epoch in 1:20) {
model %>% fit(x,y,batch_size=128,epochs = 1)
start_index <- sample(1:(nchar(text)-maxlen-1),1)
seed_text <-  str_sub(text,start_index,start_index + maxlen - 1)
if (epoch %in% c(5,10,20)){
cat("--Generating with seed:",seed_text,"\n\n")
}
for (temperature in c(0.2,0.5,1.0,1.2)) {
if (epoch %in% c(5,10,20)){
cat("--temperature:",temperature,"\n")
cat(seed_text,"\n")
}
generated_text <- seed_text
for (i in 1:100) {
sampled <- array(0,dim=c(1,maxlen,length(chars)))
generated_chars <- strsplit(generated_text,"")[[1]]
for (t in 1:length(generated_chars)){
char <- generated_chars[[t]]
sampled[1,t,char_indices[[char]]] <- 1
}
preds <- model %>% predict(sampled,verbose=0)
next_index <- sample_next_char(preds[1,],temperature)
next_char <- chars[[next_index]]
generated_text <- paste0(generated_text,next_char)
generated_text <- substring(generated_text,2)
if (epoch %in% c(5,10,20)){
cat(next_char)
}
}
if (epoch %in% c(5,10,20)){
cat("\n\n")
}
}
}
blogdown::serve_site()
blogdown::serve_site()
blogdown:::update_meta_addin()
fibonacci <- function(n) {
if(n==0){
0
} else if (n==1){
1
} else {
fibonacci(n) + fibonacci(n-1)
}
}
fibonacci(0)
fibonacci(1)
fibonacci <- function(n) {
if(n==0){
0
} else if (n==1){
1
} else {
fibonacci(n-2) + fibonacci(n-1)
}
}
fibonacci(2)
fibonacci(5)
fibonacci(6)
fibonacci(12)
fibonacci <- function(n) {
if(n==0){
x <- 0
} else if (n==1){
x <- 1
} else {
x <- fibonacci(n-2) + fibonacci(n-1)
}
return(x)
}
fibonacci(12)
knitr::opts_chunk$set(collapse = TRUE)
library(readr)
library(dplyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
data <- read_csv("seek_australia_sample.csv",locale = readr::locale(encoding = "windows-1252"))
data <- data %>%
select(job_description,category) %>%
filter(!is.na(job_description))
data_flagged <- data %>%
mutate(flag=ifelse(category %in% c("Healthcare & Medical"),"Yes","No")) %>%
mutate(no_chars=nchar(job_description))
descriptions_corpus <- VCorpus(VectorSource(data_flagged$job_description)) ##supply job descriptions into VCorpus
descriptions_corpus_clean <- tm_map(descriptions_corpus,content_transformer(tolower)) ##text transformation - lowercase
descriptions_corpus_clean <- tm_map(descriptions_corpus_clean, removeNumbers) ##remove numbers
descriptions_corpus_clean <- tm_map(descriptions_corpus_clean,removeWords, stopwords()) ##list of stopwords listed in the tm package
descriptions_corpus_clean <- tm_map(descriptions_corpus_clean, removePunctuation)
descriptions_corpus_clean <- tm_map(descriptions_corpus_clean, stripWhitespace)
descriptions_dtm <- DocumentTermMatrix(descriptions_corpus_clean) ##this will create a sparse matrix with all the words found in the above having its own column with the number of its occurrence for each job description listed in the cell
set.seed(1234501)
train_sample <- sample(4725,3780)
descriptions_dtm_train <- descriptions_dtm[train_sample,]
descriptions_dtm_test <- descriptions_dtm[-train_sample,]
descriptions_train_labels <- data_flagged[train_sample,]$flag
descriptions_test_labels <- data_flagged[-train_sample,]$flag
prop.table(table(descriptions_train_labels))
prop.table(table(descriptions_test_labels))
yes <- subset(data_flagged,flag=="Yes")
wordcloud(yes$job_description,max.words = 100,scale = c(3, 0.5),random.order = FALSE)
descriptions_freq_words <- findFreqTerms(descriptions_dtm_train, 60)
descriptions_dtm_freq_train <- descriptions_dtm_train[,descriptions_freq_words]
descriptions_dtm_freq_test <- descriptions_dtm_test[,descriptions_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
descriptions_train <- as.data.frame(apply(descriptions_dtm_freq_train, MARGIN = 2,
convert_counts))
descriptions_test <- as.data.frame(apply(descriptions_dtm_freq_test, MARGIN = 2,
convert_counts))
descriptions_classifier <- naiveBayes(descriptions_train, as.factor(descriptions_train_labels),laplace=1)
descriptions_classifier <- naiveBayes(descriptions_train, as.factor(descriptions_train_labels),laplace=1)
descriptions_test_pred <- predict(descriptions_classifier, descriptions_test, type = "raw")
descriptions_test_class <- predict(descriptions_classifier, descriptions_test)
descriptions_test_pred_yes <- as.data.frame(descriptions_test_pred) %>%
select(Yes)
CrossTable(descriptions_test_class, descriptions_test_labels,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
install.packages("irr")
library(irr)
View(descriptions_test_pred_yes)
a <- descriptions_test_class
a
test_df <- as.data.frame(cbind(descriptions_test_class,descriptions_test_labels))
View(test_df)
test_df <- as.data.frame(cbind(as.character(descriptions_test_class),descriptions_test_labels))
View(test_df)
kappa2(test_df)
blogdown::serve_site()
