<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.57.2" />


<title>Application of PCA with music features - Scott&#39;s Random Data Blog</title>
<meta property="og:title" content="Application of PCA with music features - Scott&#39;s Random Data Blog">


  <link href='/home.png' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/home.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/slee1088">GitHub</a></li>
    
    <li><a href="/reference/">References</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">2 min read</span>
    

    <h1 class="article-title">Application of PCA with music features</h1>

    
    <span class="article-date">2019-10-06</span>
    

    <div class="article-content">
      


<p><strong>Some Context</strong></p>
<p>Principal Components Analysis (PCA) is a tool that goes back decades used widely to identify patterns in data. Once patterns are discovered, one can compress the data by reducing the number of dimensions without much loss of information.</p>
<p>The objective is to transform a set of interrelated variables into a set of unrelated linear combinations of these variables (zero correlation). If one tries to apply PCA to a set of variables displaying low correlation, the analysis will most likely prove meaningless.</p>
<p>The mathematics behind PCA involves some linear algebra (primarily matrix algebra) and statistics.</p>
<p>Let <span class="math inline">\(E\)</span> be a matrix containing the eigenvectors for the covariance matrix <span class="math inline">\(\hat\Sigma\)</span> generated for our data. The eigenvectors in <span class="math inline">\(E\)</span> are our principal components. As such, these are the requirements we need them to hold, the first a necessity and the second for convenience:</p>
<ol style="list-style-type: decimal">
<li>The principal component axes are orthogonal to each other - <span class="math inline">\(e_i^Te_j = 0\)</span> when <span class="math inline">\(i \ne j\)</span></li>
<li><span class="math inline">\(e_j^Te_j = 1\)</span> <span class="math inline">\(\forall j\)</span></li>
</ol>
<p>From the theory of matrices, for any positive semidefinite matrix there exists an orthogonal matrix <span class="math inline">\(E\)</span> such that:</p>
<center>
<span class="math inline">\(\hat\Sigma E\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(E \Lambda\)</span> and <span class="math inline">\(\hat\Sigma\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(E\Lambda E^T\)</span>
</center>
<p>Here, <span class="math inline">\(\Lambda\)</span> is a diagonal matrix of non-negative values, containing the eigenvalues of <span class="math inline">\(\hat\Sigma E\)</span>. If we were to consider only the first eigenvector, the first equation becomes:</p>
<center>
<span class="math inline">\(\hat\Sigma E\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(\lambda_1 E\)</span>
</center>
<p>Once the eigenvectors are determined, we can find the principal component scores by the following transformation:</p>
<center>
<span class="math inline">\(z\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(x E\)</span>
</center>
<p>where x is a <span class="math inline">\(N\)</span> x <span class="math inline">\(e\)</span> matrix, <span class="math inline">\(N\)</span> being the number of observations and <span class="math inline">\(e\)</span> being the number of features in the dataset. We can think of <span class="math inline">\(z\)</span> as a rotation of the dataset <span class="math inline">\(x\)</span> to axes defined by <span class="math inline">\(E\)</span>.</p>
<p>The principal component scores <span class="math inline">\(z\)</span> are uncorrelated because:</p>
<center>
<span class="math inline">\(var(x)\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(var(xE)\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(E^Tvar(x)E\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(E^T\hat\Sigma E\)</span> = <span class="math inline">\(\Lambda\)</span>
</center>
<p>Recall <span class="math inline">\(\Lambda\)</span> is a <strong>diagonal</strong> matrix containing the eigenvalues of <span class="math inline">\(\hat\Sigma\)</span>.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

