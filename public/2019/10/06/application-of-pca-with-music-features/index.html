<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.57.2" />


<title>Application of PCA with audio features - Scott&#39;s Random Data Blog</title>
<meta property="og:title" content="Application of PCA with audio features - Scott&#39;s Random Data Blog">


  <link href='/home.png' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/home.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/slee1088">GitHub</a></li>
    
    <li><a href="/reference/">References</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">7 min read</span>
    

    <h1 class="article-title">Application of PCA with audio features</h1>

    
    <span class="article-date">2019-10-06</span>
    

    <div class="article-content">
      


<p><strong>Some Context</strong></p>
<p>Principal Components Analysis (PCA) is a tool that goes back decades used widely to identify patterns in data. Once patterns are discovered, one can compress the data by reducing the number of dimensions without much loss of information.</p>
<p>The objective is to transform a set of interrelated variables into a set of unrelated linear combinations of these variables. If one tries to apply PCA to a set of variables displaying low correlation, the analysis will most likely prove meaningless.</p>
<p><strong>The mathematics behind PCA involves some linear algebra (primarily matrix algebra) and statistics.</strong></p>
<p>Let <span class="math inline">\(E\)</span> be a matrix containing the eigenvectors for the covariance matrix <span class="math inline">\(\hat\Sigma\)</span> generated for our data. The eigenvectors in <span class="math inline">\(E\)</span> become our principal components (contains the loadings). These components, by virtue of them being eigenvectors hold the following properties:</p>
<ol style="list-style-type: decimal">
<li>The principal component axes are orthogonal to each other - <span class="math inline">\(e_i^Te_j = 0\)</span> when <span class="math inline">\(i \ne j\)</span></li>
<li><span class="math inline">\(e_j^Te_j = 1\)</span> <span class="math inline">\(\forall j\)</span> (this is a condition imposed out of convenience - eigenvectors do not lose direction, therefore we can scale the length of the vectors as we wish)</li>
</ol>
<p>From the theory of matrices, for any positive semidefinite matrix there exists an orthogonal matrix <span class="math inline">\(E\)</span> such that:</p>
<center>
<span class="math inline">\(\hat\Sigma E\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(E \Lambda\)</span> and <span class="math inline">\(\hat\Sigma\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(E\Lambda E^T\)</span>
</center>
<p>Here, <span class="math inline">\(\Lambda\)</span> is a diagonal matrix of non-negative values, containing the eigenvalues of <span class="math inline">\(\hat\Sigma\)</span>. If we were to consider only the first eigenvector, the first equation becomes:</p>
<center>
<span class="math inline">\(\hat\Sigma e_1\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(\lambda_1 e_1\)</span>
</center>
<p>Once the eigenvectors are determined, we can find the principal component scores by the following transformation:</p>
<center>
<span class="math inline">\(z\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(x E\)</span>
</center>
<p>where x is a <span class="math inline">\(N\)</span> x <span class="math inline">\(e\)</span> matrix, <span class="math inline">\(N\)</span> being the number of observations and <span class="math inline">\(e\)</span> being the number of features in the dataset. We can think of <span class="math inline">\(z\)</span> as a rotation of the dataset <span class="math inline">\(x\)</span> to axes defined by <span class="math inline">\(E\)</span>.</p>
<p>The principal component scores <span class="math inline">\(z\)</span> are uncorrelated because</p>
<center>
<span class="math inline">\(var(z)\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(var(xE)\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(E^Tvar(x)E\)</span> <span class="math inline">\(=\)</span> <span class="math inline">\(E^T\hat\Sigma E\)</span> = <span class="math inline">\(\Lambda\)</span>
</center>
<p>Recall <span class="math inline">\(\Lambda\)</span> is a <strong>diagonal</strong> matrix containing the eigenvalues of <span class="math inline">\(\hat\Sigma\)</span>.</p>
<p>Using the eigenvalues, we can now determine which eigenvectors (components) we want to use or more importantly which ones to remove. A judgement call needs to be made with the help of the latent roots criterion and the scree test. You will inevitably lose some information by culling components but if the eigenvalues are small, you won’t lose much.</p>
<p>For a thorough commentary, please refer to the following <a href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf">link</a>.</p>
<p><strong>Workflow</strong></p>
<ul>
<li><p>The dataset I’ve used is Spotify data available on <a href="https://www.kaggle.com/jsongunsw/spotify-datasets/data">Kaggle</a> which contains audio features for the top 100 spotify tracks at a certain period in time. The data seems fairly clean with all records containing values.</p></li>
<li><p>Read in the data and plot the correlation of the audio features.</p></li>
</ul>
<pre class="r"><code>data &lt;- read_csv(&quot;featuresdf.csv&quot;,locale = readr::locale(encoding = &quot;windows-1252&quot;))
## Parsed with column specification:
## cols(
##   id = col_character(),
##   name = col_character(),
##   artists = col_character(),
##   danceability = col_double(),
##   energy = col_double(),
##   key = col_double(),
##   loudness = col_double(),
##   mode = col_double(),
##   speechiness = col_double(),
##   acousticness = col_double(),
##   instrumentalness = col_double(),
##   liveness = col_double(),
##   valence = col_double(),
##   tempo = col_double(),
##   duration_ms = col_double(),
##   time_signature = col_double()
## )
data_scaled &lt;- as.data.frame(scale(data[, 4:13]))
plot_correlation(data_scaled)</code></pre>
<p><img src="/post/2019-10-06-application-of-pca-with-music-features_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<ul>
<li><p>Based off the correlation matrix above, we don’t see a high correlation between the variables besides energy and loudness. This is looking bad for our PCA. Based off the above, we can suspect that we will need the majority of the components to explain the majority of the variance in the dataset. We can see this in the scree-plot below. Instead of one or two components that can explain the majority of the variance in the dataset, it looks like we need at least a half of the components.</p></li>
<li><p>Apply the principal() function from the <em>psych</em> package.</p></li>
</ul>
<pre class="r"><code>pca &lt;- principal(data[, 4:13], rotate = &quot;none&quot;)
plot(pca$values[1:10], type = &quot;b&quot;, ylab = &quot;Eigenvalues&quot;, xlab = &quot;Component&quot;)</code></pre>
<p><img src="/post/2019-10-06-application-of-pca-with-music-features_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>pca_5 &lt;- principal(data_scaled, nfactors = 5, rotate = &quot;none&quot;)
pca_5
## Principal Components Analysis
## Call: principal(r = data_scaled, nfactors = 5, rotate = &quot;none&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##                    PC1   PC2   PC3   PC4   PC5   h2   u2 com
## danceability      0.16  0.75  0.11  0.39  0.17 0.79 0.21 1.8
## energy            0.83 -0.24  0.09  0.06  0.07 0.76 0.24 1.2
## key              -0.04 -0.06  0.64 -0.42  0.14 0.60 0.40 1.9
## loudness          0.91 -0.05 -0.05 -0.11 -0.06 0.84 0.16 1.1
## mode             -0.15 -0.03 -0.64  0.21 -0.29 0.56 0.44 1.8
## speechiness      -0.50  0.17  0.40  0.47  0.05 0.66 0.34 3.2
## acousticness     -0.22  0.49 -0.28 -0.55 -0.17 0.70 0.30 3.1
## instrumentalness  0.03 -0.25 -0.36  0.19  0.74 0.78 0.22 1.9
## liveness          0.17 -0.29  0.19  0.40 -0.54 0.60 0.40 3.0
## valence           0.59  0.63 -0.01  0.08 -0.02 0.75 0.25 2.0
## 
##                        PC1  PC2  PC3  PC4  PC5
## SS loadings           2.23 1.45 1.23 1.12 1.02
## Proportion Var        0.22 0.14 0.12 0.11 0.10
## Cumulative Var        0.22 0.37 0.49 0.60 0.71
## Proportion Explained  0.32 0.21 0.17 0.16 0.14
## Cumulative Proportion 0.32 0.52 0.70 0.86 1.00
## 
## Mean item complexity =  2.1
## Test of the hypothesis that 5 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.11 
##  with the empirical chi square  111.85  with prob &lt;  1.7e-22 
## 
## Fit based upon off diagonal values = 0.62</code></pre>
<ul>
<li><p>We can see that the first component has a high loading for energy and loudness which is what we were expecting given their high correlation.</p></li>
<li><p>Before we investigate further, let us apply orthogonal rotation on our components using varimax. The purpose of rotating is to maximise the loadings on particular features on a specific component. This helps interpretation.</p></li>
</ul>
<pre class="r"><code>pca_rotate &lt;- principal(data_scaled, nfactors = 5, rotate = &quot;varimax&quot;)
pca_rotate
## Principal Components Analysis
## Call: principal(r = data_scaled, nfactors = 5, rotate = &quot;varimax&quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##                    RC1   RC2   RC4   RC3   RC5   h2   u2 com
## danceability     -0.13  0.88 -0.01  0.01 -0.04 0.79 0.21 1.1
## energy            0.76  0.11  0.38 -0.13 -0.06 0.76 0.24 1.6
## key              -0.03 -0.13 -0.07 -0.75  0.11 0.60 0.40 1.1
## loudness          0.89  0.19  0.13 -0.04  0.06 0.84 0.16 1.1
## mode             -0.06 -0.11 -0.08  0.73  0.03 0.56 0.44 1.1
## speechiness      -0.71  0.23  0.30 -0.12  0.06 0.66 0.34 1.7
## acousticness     -0.06  0.06 -0.81  0.08  0.19 0.70 0.30 1.2
## instrumentalness  0.04 -0.05  0.13  0.13 -0.86 0.78 0.22 1.1
## liveness          0.08 -0.08  0.57  0.18  0.49 0.60 0.40 2.3
## valence           0.41  0.74 -0.12  0.01  0.11 0.75 0.25 1.7
## 
##                        RC1  RC2  RC4  RC3  RC5
## SS loadings           2.07 1.46 1.27 1.19 1.05
## Proportion Var        0.21 0.15 0.13 0.12 0.11
## Cumulative Var        0.21 0.35 0.48 0.60 0.71
## Proportion Explained  0.29 0.21 0.18 0.17 0.15
## Cumulative Proportion 0.29 0.50 0.68 0.85 1.00
## 
## Mean item complexity =  1.4
## Test of the hypothesis that 5 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.11 
##  with the empirical chi square  111.85  with prob &lt;  1.7e-22 
## 
## Fit based upon off diagonal values = 0.62</code></pre>
<ul>
<li><p>Now, for the first component, we can more easily determine what features are being emphasised (energy, loudness, valence and speechiness). With business domain knowledge, one can now try to categorise and define each of the components, not in terms of just one variable but by grouping variables, for e.g. energy and loudness are quite similar and can be coined as ambience for instance.</p></li>
<li><p>To finish off, let’s check out the correlation of our components (for both the rotated and not rotated components). They should be uncorrelated except for themselves.</p></li>
</ul>
<pre class="r"><code>pca_scores &lt;- data.frame(round(pca_rotate$scores, digits = 2))
plot_correlation(pca_scores)</code></pre>
<p><img src="/post/2019-10-06-application-of-pca-with-music-features_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>pca_5_scores &lt;- data.frame(round(pca_5$scores, digits = 2))
plot_correlation(pca_5_scores)</code></pre>
<p><img src="/post/2019-10-06-application-of-pca-with-music-features_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

